#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TÃ©lÃ©chargement complet de TOUTES les conventions collectives
"""
import os
import json
import requests
import csv
from pathlib import Path
from datetime import datetime
import concurrent.futures as cf
from typing import Tuple, List
import time

def sanitize_filename(name: str) -> str:
    """Nettoie le nom de fichier pour le systÃ¨me de fichiers"""
    for char in '<>:"|?*\\/':
        name = name.replace(char, '_')
    return name.strip()

def clean_url(url: str) -> str:
    """Nettoie les URLs malformÃ©es"""
    if url.startswith('https://,https://'):
        return url.replace('https://,https://', 'https://')
    elif url.startswith('http://,http://'):
        return url.replace('http://,http://', 'http://')
    return url.strip()

def download_one(session: requests.Session, convention: dict, out_dir: Path) -> Tuple[str, str, dict]:
    """TÃ©lÃ©charge une convention. Retourne (status, message, convention)"""
    try:
        raw_url = convention.get('Link', '').strip()
        if not raw_url:
            return ("ERROR", "URL manquante", convention)
        
        # Nettoyer l'URL
        url = clean_url(raw_url)
        
        # Nom de fichier
        nom = convention.get('Nom De la Convention', 'Convention')
        idcc = convention.get('IDCC', 'Sans_IDCC')
        filename = sanitize_filename(f"{idcc}_{nom}.pdf")
        filepath = out_dir / filename
        
        # VÃ©rifier si dÃ©jÃ  tÃ©lÃ©chargÃ©
        if filepath.exists() and filepath.stat().st_size > 1000:
            return ("SKIP", f"DÃ©jÃ  prÃ©sent: {filename}", convention)
        
        # TÃ©lÃ©charger avec retry automatique
        max_retries = 3
        response = None
        for attempt in range(max_retries):
            try:
                response = session.get(url, timeout=45, stream=True)
                if response.status_code == 200:
                    break
                elif response.status_code in [429, 503, 504]:  # Rate limit ou serveur occupÃ©
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)  # Backoff exponentiel
                        continue
                return ("ERROR", f"HTTP {response.status_code} aprÃ¨s {max_retries} tentatives", convention)
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                return ("ERROR", "Timeout aprÃ¨s plusieurs tentatives", convention)
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                return ("ERROR", f"Erreur rÃ©seau: {str(e)}", convention)
        
        if response is None or response.status_code != 200:
            return ("ERROR", "Impossible d'obtenir une rÃ©ponse valide", convention)
        
        # Sauvegarder
        tmp_filepath = filepath.with_suffix('.tmp')
        try:
            with open(tmp_filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=16384):
                    if chunk:
                        f.write(chunk)
            
            size = tmp_filepath.stat().st_size
            if size < 1000:
                tmp_filepath.unlink(missing_ok=True)
                return ("ERROR", f"Fichier trop petit ({size} octets)", convention)
            
            # Renommer une fois terminÃ©
            tmp_filepath.rename(filepath)
            return ("SUCCESS", f"TÃ©lÃ©chargÃ©: {filename} ({size:,} octets)", convention)
            
        except Exception as e:
            tmp_filepath.unlink(missing_ok=True)
            return ("ERROR", f"Erreur sauvegarde: {str(e)}", convention)
        
    except Exception as e:
        return ("ERROR", f"Erreur gÃ©nÃ©rale: {str(e)}", convention)

def main():
    # Configuration pour tÃ©lÃ©chargement complet
    MAX_WORKERS = 6  # ParallÃ©lisme modÃ©rÃ© pour Ã©viter la surcharge serveur
    
    # Dossier de sortie
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_dir = Path(f"resultats_telechargements/complet_{timestamp}")
    out_dir.mkdir(parents=True, exist_ok=True)
    
    # Journal CSV
    log_file = out_dir / "journal_complet.csv"
    progress_file = out_dir / "progression.txt"
    
    print(f"ğŸ“ Dossier: {out_dir}")
    print(f"âš¡ ParallÃ©lisme: {MAX_WORKERS} workers")
    print(f"ğŸ¯ Objectif: TOUTES les conventions disponibles")
    
    # Charger le JSON
    try:
        with open("conventions_collectives_integrales_lienpdf_nettoye.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"ğŸ“„ JSON chargÃ©: {len(data)} conventions Ã  tÃ©lÃ©charger")
    except Exception as e:
        print(f"âŒ Erreur JSON: {e}")
        return
    
    start_time = time.time()
    
    # Session HTTP optimisÃ©e
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    })
    
    # TÃ©lÃ©chargement parallÃ¨le
    results = {"SUCCESS": 0, "ERROR": 0, "SKIP": 0}
    processed = 0
    total = len(data)
    
    # Initialiser les fichiers de suivi
    with open(log_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile, delimiter=';')
        writer.writerow(['horodatage', 'statut', 'message', 'idcc', 'nom', 'url'])
        
        with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Soumettre toutes les tÃ¢ches
            futures = [
                executor.submit(download_one, session, conv, out_dir) 
                for conv in data
            ]
            
            # Traiter les rÃ©sultats au fur et Ã  mesure
            for future in cf.as_completed(futures):
                status, message, conv = future.result()
                results[status] += 1
                processed += 1
                
                # Log CSV
                writer.writerow([
                    datetime.now().isoformat(),
                    status,
                    message,
                    conv.get('IDCC', ''),
                    conv.get('Nom De la Convention', ''),
                    conv.get('Link', '')
                ])
                csvfile.flush()
                
                # Progression toutes les 10 conventions
                if processed % 10 == 0 or processed == total:
                    elapsed = time.time() - start_time
                    rate = processed / elapsed if elapsed > 0 else 0
                    eta = (total - processed) / rate if rate > 0 else 0
                    
                    progress = f"[{processed:3d}/{total}] âœ…{results['SUCCESS']} â­ï¸{results['SKIP']} âŒ{results['ERROR']} | {rate:.1f}/s | ETA: {eta/60:.1f}min"
                    print(progress)
                    
                    # Sauvegarder progression
                    with open(progress_file, 'w') as pf:
                        pf.write(f"{progress}\nDerniÃ¨re MAJ: {datetime.now().isoformat()}\n")
                
                # Affichage dÃ©taillÃ© pour erreurs importantes
                if status == "ERROR" and processed % 50 == 0:
                    print(f"âŒ ERREUR rÃ©cente: {conv.get('IDCC', 'N/A')} - {message[:80]}...")
    
    # RÃ©sumÃ© final
    elapsed = time.time() - start_time
    print(f"\nğŸ‰ TÃ‰LÃ‰CHARGEMENT TERMINÃ‰ en {elapsed/60:.1f} minutes")
    print(f"ğŸ“Š RÃ‰SULTATS FINAUX:")
    print(f"  âœ… SuccÃ¨s: {results['SUCCESS']}")
    print(f"  â­ï¸  IgnorÃ©s: {results['SKIP']}")
    print(f"  âŒ Erreurs: {results['ERROR']}")
    print(f"  ğŸ“ˆ Taux de succÃ¨s: {results['SUCCESS']/total*100:.1f}%")
    
    # Statistiques finales
    pdf_files = list(out_dir.glob("*.pdf"))
    total_size = sum(f.stat().st_size for f in pdf_files)
    print(f"  ğŸ“‹ Fichiers PDF: {len(pdf_files)}")
    print(f"  ğŸ’¾ Taille totale: {total_size/1024/1024/1024:.2f} GB")
    print(f"  ğŸ“ Dossier: {out_dir}")
    print(f"  ğŸ“„ Journal: {log_file}")
    
    # Sauvegarder rÃ©sumÃ© final
    with open(out_dir / "resume_final.txt", 'w', encoding='utf-8') as f:
        f.write(f"TÃ©lÃ©chargement complet terminÃ© le {datetime.now().isoformat()}\n")
        f.write(f"DurÃ©e: {elapsed/60:.1f} minutes\n")
        f.write(f"SuccÃ¨s: {results['SUCCESS']}/{total} ({results['SUCCESS']/total*100:.1f}%)\n")
        f.write(f"Fichiers: {len(pdf_files)} PDFs\n")
        f.write(f"Taille: {total_size/1024/1024/1024:.2f} GB\n")

if __name__ == "__main__":
    main()